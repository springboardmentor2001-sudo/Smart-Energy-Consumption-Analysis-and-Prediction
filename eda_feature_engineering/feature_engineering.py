# -*- coding: utf-8 -*-
"""Feature Engineering

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UTNSMQbrs2C8Wt3ZUeYFp_RIuuiPMHWN

# Feature Engineering

We perform feature engineering to extract more useful information from the existing columns and create new features that might improve model performance. This includes:
- Converting 'Timestamp' to datetime objects and extracting 'Hour', 'Day', 'Month'.
- Creating an 'IsWeekend' flag.
- Mapping categorical 'HVACUsage', 'LightingUsage', and 'Holiday' to numerical values (0 or 1).
- Creating interaction features like 'Temp_Occupancy' and 'Area_Occupancy'.
- Calculating 'Energy_per_Person' and 'Renewable_Ratio'.
- One-hot encoding 'DayOfWeek'.
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('Energy_consumption.csv')

df['Timestamp'] = pd.to_datetime(df['Timestamp'])
df["Hour"] = df["Timestamp"].dt.hour
df["Day"] = df["Timestamp"].dt.day
df["Month"] = df["Timestamp"].dt.month
df["IsWeekend"] = df["DayOfWeek"].isin(["Saturday", "Sunday"]).astype(int)
df["HVACUsage"] = df["HVACUsage"].map({"Off": 0, "On": 1})
df["LightingUsage"] = df["LightingUsage"].map({"Off": 0, "On": 1})
df["Holiday"] = df["Holiday"].map({"No": 0, "Yes": 1})
df["Temp_Occupancy"] = df["Temperature"] * df["Occupancy"]
df["Area_Occupancy"] = df["SquareFootage"] * df["Occupancy"]
df["Energy_per_Person"] = df["EnergyConsumption"] / (df["Occupancy"] + 1)
df["Renewable_Ratio"] = df["RenewableEnergy"] / (df["EnergyConsumption"] + 1)
df = pd.get_dummies(df, columns=["DayOfWeek"], drop_first=True)

df.head()

df.drop('Timestamp', axis = 1, inplace = True)

df.nunique().sort_values()

binary_features = ["HVACUsage", "LightingUsage", "Holiday", "IsWeekend"]

for col in binary_features:
    print(f"\n{col} distribution:")
    print(df[col].value_counts(normalize=True))

"""#### Feature Selection using Random Forest

To reduce dimensionality and potentially improve model efficiency, we use a Random Forest model to identify and remove unimportant features. Features with an importance score below a certain threshold (0.01 in this case) are dropped.
"""

X = df.drop(columns=["EnergyConsumption", "Timestamp"], errors="ignore")
y = df["EnergyConsumption"]

from sklearn.ensemble import RandomForestRegressor
import pandas as pd

rf = RandomForestRegressor(n_estimators=500,
                           random_state=42,
                           n_jobs=-1)
rf.fit(X, y)

feature_importance = pd.Series(rf.feature_importances_,index=X.columns).sort_values(ascending=False)
feature_importance

IMPORTANCE_THRESHOLD = 0.01
unimportant_features = feature_importance[feature_importance < IMPORTANCE_THRESHOLD].index.tolist()

unimportant_features

df_reduced = df.drop(columns=unimportant_features)

print("Original features:", X.shape[1])
print("Remaining features:", df_reduced.shape[1])
print(df_reduced)

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

X = df.drop(columns=["EnergyConsumption", "Timestamp"], errors="ignore")
y = df["EnergyConsumption"]

"""#### Model Training and Evaluation Setup

We import various regression models and metrics from scikit-learn. The data is split into training and testing sets. A custom `evaluate` function is defined to streamline the process of training a model and calculating key regression metrics (MAE, MSE, RMSE, R2).
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_reduced = X.drop(columns=unimportant_features)
X_train_r = X_train.drop(columns=unimportant_features)
X_test_r  = X_test.drop(columns=unimportant_features)

from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    root_mean_squared_error
)


def evaluate(model, X_train, X_test, y_train, y_test, name):
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    return {"Model": name,
            "MAE": mean_absolute_error(y_test, preds),
            "MSE": mean_squared_error(y_test, preds),
            "RMSE": root_mean_squared_error(y_test, preds),
            "R2": r2_score(y_test, preds)}

"""#### Compare Model Performance

We train and evaluate several regression models (Linear Regression, Decision Tree, Random Forest, XGBoost, LightGBM) on both the full feature set and the reduced feature set. The results are collected and displayed in a DataFrame, sorted by R2 score to identify the best performing model.
"""

results = []

models = [("Linear Regression", LinearRegression()),
          ("Decision Tree", DecisionTreeRegressor(random_state=42)),
          ("Random Forest", RandomForestRegressor(n_estimators=300, random_state=42)),
          ("XGBoost", XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=5, random_state=42)),
          ("LightGBM", LGBMRegressor(n_estimators=300, learning_rate=0.05, random_state=42))]

for name, model in models:
    results.append(evaluate(model, X_train, X_test, y_train, y_test, f"{name} (Full)"))
    results.append(evaluate(model, X_train_r, X_test_r, y_train, y_test, f"{name} (Reduced)"))

results_df = pd.DataFrame(results)
results_df

results_df.sort_values(by="R2", ascending=False)

"""#### Final Model Training and Persistence

Based on the evaluation, LightGBM with the reduced feature set was selected as the best performing model. This model is trained on the entire dataset (using the reduced features) and then saved using `pickle` for future use. A sample prediction is also made to demonstrate its functionality.
"""

from lightgbm import LGBMRegressor

X_final = X_reduced
y_final = y

final_model = LGBMRegressor(n_estimators=300, learning_rate=0.05, random_state=42)
final_model.fit(X_final, y_final)

model_package = {"model": final_model, "features": X_final.columns.tolist()}

import pickle

with open("smart_energy_model.pkl", "wb") as f:
    pickle.dump(model_package, f)

with open("smart_energy_model.pkl", "rb") as f:
    loaded_package = pickle.load(f)

loaded_model = loaded_package["model"]
feature_names = loaded_package["features"]

sample_input = X_final.iloc[:1]
prediction = loaded_model.predict(sample_input)

print("Prediction:", prediction)

"""#### K-Fold Cross-Validation for Robust Evaluation

To provide a more robust and reliable evaluation of the selected LightGBM model, K-Fold cross-validation is performed. This method splits the data into K folds, and for each fold, one part is used for testing and the remaining K-1 parts for training. The average of the evaluation metrics across all folds gives a more stable estimate of the model's performance.
"""

from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from lightgbm import LGBMRegressor

df = pd.read_csv('Energy_consumption.csv')

df['Timestamp'] = pd.to_datetime(df['Timestamp'])
df["Hour"] = df["Timestamp"].dt.hour
df["Day"] = df["Timestamp"].dt.day
df["Month"] = df["Timestamp"].dt.month
df["IsWeekend"] = df["DayOfWeek"].isin(["Saturday", "Sunday"]).astype(int)
df["HVACUsage"] = df["HVACUsage"].map({"Off": 0, "On": 1})
df["LightingUsage"] = df["LightingUsage"].map({"Off": 0, "On": 1})
df["Holiday"] = df["Holiday"].map({"No": 0, "Yes": 1})
df["Temp_Occupancy"] = df["Temperature"] * df["Occupancy"]
df["Area_Occupancy"] = df["SquareFootage"] * df["Occupancy"]
df["Energy_per_Person"] = df["EnergyConsumption"] / (df["Occupancy"] + 1)
df["Renewable_Ratio"] = df["RenewableEnergy"] / (df["EnergyConsumption"] + 1)
df = pd.get_dummies(df, columns=["DayOfWeek"], drop_first=True)

df.drop('Timestamp', axis = 1, inplace = True)

X = df.drop(columns=["EnergyConsumption"], errors="ignore")
y = df["EnergyConsumption"]

rf = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)
rf.fit(X, y)
feature_importance = pd.Series(rf.feature_importances_,index=X.columns).sort_values(ascending=False)
IMPORTANCE_THRESHOLD = 0.01
unimportant_features = feature_importance[feature_importance < IMPORTANCE_THRESHOLD].index.tolist()

X_reduced = X.drop(columns=unimportant_features)

X_final = X_reduced
y_final = y

kf = KFold(n_splits=5, shuffle=True, random_state=42)

mae_scores = []
mse_scores = []
rmse_scores = []
r2_scores = []

for train_index, test_index in kf.split(X_final):
    X_train_cv, X_test_cv = X_final.iloc[train_index], X_final.iloc[test_index]
    y_train_cv, y_test_cv = y_final.iloc[train_index], y_final.iloc[test_index]

    model = LGBMRegressor(n_estimators=300, learning_rate=0.05, random_state=42)
    model.fit(X_train_cv, y_train_cv)
    preds = model.predict(X_test_cv)

    mae_scores.append(mean_absolute_error(y_test_cv, preds))
    mse_scores.append(mean_squared_error(y_test_cv, preds))
    rmse_scores.append(root_mean_squared_error(y_test_cv, preds))
    r2_scores.append(r2_score(y_test_cv, preds))

print(f"Average MAE: {np.mean(mae_scores):.4f}")
print(f"Average MSE: {np.mean(mse_scores):.4f}")
print(f"Average RMSE: {np.mean(rmse_scores):.4f}")
print(f"Average R2: {np.mean(r2_scores):.4f}")
