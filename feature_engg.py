# -*- coding: utf-8 -*-
"""feature engg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MhihwZnScfJHoqULk0GEjH0qWhkg2zCK
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("Energy_consumption.csv")

df.head()

df.tail()

df.shape

df.columns

df.info()

df.isnull().sum()

df.duplicated().sum()

"""we have no Null VALUES in our dataset and No duplicate Value

Mean,Standard deviation,Minimum & maximum values,Quartiles
"""



"""Graphical Exploratory Data Analysis"""

df.hist(figsize=(12,8))
plt.show()

"""Histograms show how energy consumption values are distributed"""

plt.figure(figsize=(10,5))
sns.boxplot(data=df)
plt.show()

"""correlation Heatmap"""

numeric_df = df.select_dtypes(include=['number'])
plt.figure(figsize=(10,6))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')
plt.show()

"""Energy Consumption Prediction with Advanced Feature Engineering and HVAC Modeling

OBJECTIVE OF THE PROJECT

The objective of this project is to accurately predict energy consumption by engineering meaningful features from temporal, historical, and weather-related data, with special focus on HVAC-driven energy usage using Heating and Cooling Degree Days.

Datetime Processing:- Datetime conversion enables extraction of seasonal, weekly, and daily usage patterns, which are crucial for energy modeling.
"""

df['Timestamp'] = pd.to_datetime(df['Timestamp'])

"""COMPREHENSIVE FEATURE ENGINEERING

1. Calendar & Time-Based Features
"""

df['Year'] = df['Timestamp'].dt.year
df['Month'] = df['Timestamp'].dt.month
df['Day'] = df['Timestamp'].dt.day
df['Weekday'] = df['Timestamp'].dt.dayofweek
df['Week'] = df['Timestamp'].dt.isocalendar().week.astype(int)
df['Is_Weekend'] = (df['Weekday'] >= 5).astype(int)

"""2.Cyclical Encoding (Seasonality Preservation)"""

df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)
df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)

df['Weekday_sin'] = np.sin(2 * np.pi * df['Weekday'] / 7)
df['Weekday_cos'] = np.cos(2 * np.pi * df['Weekday'] / 7)

"""HVAC Base Temperatures"""

HEATING_BASE = 18
COOLING_BASE = 22

"""Heating Degree Days (HDD):-Measures heating demand when temperature drops."""

HEATING_BASE = 18
df['HDD'] = np.maximum(0, HEATING_BASE - df['Temperature'])

df.head()

"""Cooling Degree Days (CDD):- Measures air-conditioning demand when temperature rises"""

df['CDD'] = np.maximum(0, df['Temperature'] - COOLING_BASE)

df.head()

"""HVAC Binary Indicators:-Identifies periods when HVAC systems are active."""

df['Heating_On'] = (df['HDD'] > 0).astype(int)
df['Cooling_On'] = (df['CDD'] > 0).astype(int)

df.head()

"""HVAC Intensity Features:- Captures non-linear HVAC energy growth."""

df['HDD_Squared'] = df['HDD'] ** 2
df['CDD_Squared'] = df['CDD'] ** 2

"""Historical Consumption (Lag Features):-Energy demand depends heavily on past behavior."""

df['Lag_1'] = df['EnergyConsumption'].shift(1)
df['Lag_7'] = df['EnergyConsumption'].shift(7)
df['Lag_14'] = df['EnergyConsumption'].shift(14)
df['Lag_30'] = df['EnergyConsumption'].shift(30)
# Removed fillna for lag features to prevent data leakage

df.head()

"""Rolling Statistical Features:-Captures short-term and long-term trends."""

df['Rolling_mean_7'] = df['EnergyConsumption'].rolling(7).mean()
df['Rolling_std_7'] = df['EnergyConsumption'].rolling(7).std()

df['Rolling_mean_30'] = df['EnergyConsumption'].rolling(30).mean()
df['Rolling_max_30'] = df['EnergyConsumption'].rolling(30).max()
# Removed fillna for rolling features to prevent data leakage

df.head()

"""Consumption Dynamics :- Identifies sudden spikes and drops."""

df['Consumption_Change'] = df['EnergyConsumption'].diff()
df['Consumption_Pct_Change'] = df['EnergyConsumption'].pct_change()

df.head()

"""Interaction Features :- Models behavioral + weather interaction."""

df['CDD_Weekend'] = df['CDD'] * df['Is_Weekend']
df['HDD_Weekend'] = df['HDD'] * df['Is_Weekend']

df['Temp_Month'] = df['Temperature'] * df['Month']

"""Remove Missing Values"""

# df.dropna(inplace=True) # This is now moved to after all lag features are created to prevent leakage

"""Define Features & Target"""

X = df.drop(columns=['EnergyConsumption', 'Timestamp'])
y = df['EnergyConsumption']

"""FEATURE SELECTION (IMPORTANT)

1. Correlation-Based Filtering :- Removes weak predictors.
"""

numeric_df = df.select_dtypes(include=['number'])
corr = numeric_df.corr()['EnergyConsumption'].abs()
selected_features = corr[corr > 0.3].index
X = df[selected_features.drop('EnergyConsumption')]

"""2. Random Forest Feature Importance :-Keeps only high-impact features."""

from sklearn.ensemble import RandomForestRegressor

rf_selector = RandomForestRegressor(random_state=42)
rf_selector.fit(X, y)

importance = pd.Series(rf_selector.feature_importances_, index=X.columns)
important_features = importance[importance > 0.025].index

X = X[important_features]

"""Train–Test Split"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""Feature Scaling"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

df['Hour'] = df['Timestamp'].dt.hour
df['IsWeekend'] = df['DayOfWeek'].isin(['Saturday','Sunday']).astype(int)

"""Interaction Features"""

df['Temp_Humidity_Index'] = df['Temperature'] * df['Humidity']

"""efficiency based features"""

df['Energy_per_sqft'] = df['EnergyConsumption'] / df['SquareFootage']

"""Encoding Strategy"""

from sklearn.preprocessing import OneHotEncoder

OneHotEncoder(handle_unknown='ignore')



"""Define Features (X) and Target (y)"""

X = df.drop(['EnergyConsumption', 'Timestamp'], axis=1)

y = df['EnergyConsumption']

"""Temperature Intensity Features"""

df['Temp_Squared'] = df['Temperature'] ** 2
df['Temp_Cube'] = df['Temperature'] ** 3

"""Temperature + Time Interaction"""

df['Temp_Hour_Interaction'] = df['Temperature'] * df['Hour']

"""Holiday & Weekend Combined Effect

"""

df['Holiday_Weekend'] = (
    (df['Holiday'] == 'Yes') | (df['IsWeekend'] == 1)
).astype(int)

"""Energy usage behavior on holidays/weekends is similar

: Lag Feature
"""

df['Prev_Hour_Energy'] = df['EnergyConsumption'].shift(1)
# Removed fillna for Prev_Hour_Energy to prevent data leakage

df.head()

"""TIME DEPENDENCY"""

df = df.sort_values('Timestamp')

# Drop NaNs after all lag features have been created
df.dropna(inplace=True)

split_index = int(len(df) * 0.8)

train_df = df.iloc[:split_index]
test_df  = df.iloc[split_index:]

X_train = train_df.drop('EnergyConsumption', axis=1)
y_train = train_df['EnergyConsumption']

X_test  = test_df.drop('EnergyConsumption', axis=1)
y_test  = test_df['EnergyConsumption']

# Explicitly drop 'Timestamp' column from X_train and X_test
X_train = X_train.drop('Timestamp', axis=1)
X_test = X_test.drop('Timestamp', axis=1)

"""ADD STRONG LAG FEATURES"""

from lightgbm import LGBMRegressor

model = LGBMRegressor(
    n_estimators=800,
    learning_rate=0.02,
    max_depth=8,
    num_leaves=64,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

"""USE CROSS-VALIDATION"""

from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

tscv = TimeSeriesSplit(n_splits=5)

# Identify feature types for the ColumnTransformer
current_X_train_columns = X_train.columns

# 'Timestamp' is now dropped from X_train/X_test, so features_to_drop is empty
features_to_drop = []
categorical_features = [col for col in ['HVACUsage', 'LightingUsage', 'DayOfWeek', 'Holiday'] if col in current_X_train_columns]
numerical_features = [col for col in current_X_train_columns if col not in features_to_drop and col not in categorical_features]

# Create the preprocessor using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ],
    remainder='passthrough'
)

# Re-instantiate models to ensure fresh state
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42),
    "XGBoost": XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42),
    "LightGBM": LGBMRegressor(n_estimators=200, learning_rate=0.05, random_state=42)
}

# We'll use LightGBM for CV for demonstration, as 'best_model' is determined later.
# This ensures this cell can run independently after data preparation.
best_model_instance_for_cv = models['LightGBM']

# Create a pipeline for cross-validation with the chosen model
pipeline = Pipeline([
    ('preprocessing', preprocessor),
    ('model', best_model_instance_for_cv)
])

sync_outputs = False
scores = cross_val_score(
    pipeline, X_train, y_train,
    cv=tscv, scoring='r2'
)

print("Mean CV R2:", scores.mean())

"""MODEL TRAINING

Import Models & Metrics
"""

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import pandas as pd

"""Create All Models"""

models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42),
    "XGBoost": XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42),
    "LightGBM": LGBMRegressor(n_estimators=200, learning_rate=0.05, random_state=42)
}

"""Create an Evaluation Function"""

def train_and_evaluate(model):

    pipeline = Pipeline([
        ('preprocessing', preprocessor),
        ('model', model)
    ])

    pipeline.fit(X_train, y_train)

    y_pred = pipeline.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    return mae, rmse, r2

"""Train Each Model One by One"""

from sklearn.preprocessing import FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Identify feature types for the ColumnTransformer
current_X_train_columns = X_train.columns

features_to_drop = []
categorical_features = [col for col in ['HVACUsage', 'LightingUsage', 'DayOfWeek', 'Holiday'] if col in current_X_train_columns]
numerical_features = [col for col in current_X_train_columns if col not in features_to_drop and col not in categorical_features]

# Create the preprocessor using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ],
    remainder='passthrough'
)

results = {}

for model_name, model in models.items():

    mae, rmse, r2 = train_and_evaluate(model)

    results[model_name] = [mae, rmse, r2]

"""Create Comparison Table"""

results_df = pd.DataFrame(
    results,
    index=["MAE", "RMSE", "R2 Score"]
).T

results_df

"""Identify the Best Model"""

best_model = results_df['R2 Score'].idxmax()
best_score = results_df.loc[best_model, 'R2 Score']

print("Best Performing Model:", best_model)
print("Best R2 Score:", best_score)

"""Project Summary

The goal of this project was to predict energy consumption using machine learning.
After analyzing the dataset, weak and noisy features were removed, and strong feature engineering was applied, including temporal features, interaction features, and lag-based features to capture time dependency.

A time-based 80:20 train–test split was used to avoid data leakage. Multiple models were trained and compared using MAE, RMSE, and R². Among all models, LightGBM performed the best, showing strong generalization and stability.

The final model was trained on ~15–18 engineered features and achieved the highest predictive accuracy.
"""